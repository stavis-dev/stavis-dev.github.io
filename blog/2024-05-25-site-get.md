---
title: Скачивание сайтов утилитой wget
description: Как с помпощью утилиты wget скачивать сайты целиком
authors:
  - stavis
tags:
  - wget
  - linux
  - web
---
# Качаем сайты целиком
Для работы бывали случаи, когда приходилось скачивать сайты целиком. Не просто сохранить страничку в браузере, а скачать со всеми файлами стилей, js скриптами, картинками.  
Возможно для такого случая существуют специальные программы. Но как оказалось есть более универсальный методы например использовать утилиту wget. В общем это памятка как раз по использованию это утилиты.
<!--truncate-->
## Скачивание сайта утилитой

```bash
wget -p -k http://somewebsite.com

# мой паттерн from https://nborzenko.com/
wget -e robots=off -r -np --page-requisites --convert-links http://somewebsite.com
wget -l 1 -e robots=off -r -np --page-requisites --convert-links http://somewebsite.com
```

Рассмотрим используемые параметры:

`-r` — указывает на то, что нужно рекурсивно переходить по ссылкам на сайте, чтобы скачивать страницы.

`-k`, `--convert-links` — используется для того, чтобы wget преобразовал все ссылки в скаченных файлах таким образом, чтобы по ним можно было переходить на локальном компьютере (в автономном режиме).

`-p`, `--page-requisites` — указывает на то, что нужно загрузить все файлы, которые требуются для отображения страниц (изображения, css и т.д.).

`-l` — определяет максимальную глубину вложенности страниц, которые wget должен скачать (по умолчанию значение равно 5, в примере мы установили 1). В большинстве случаев сайты имеют страницы с большой степенью вложенности и wget может просто «закопаться», скачивая новые страницы. Чтобы этого не произошло можно использовать параметр -l.

`-E` — добавлять к загруженным файлам расширение .html.

`-nc` — при использовании данного параметра существующие файлы не будут перезаписаны. Это удобно, когда нужно продолжить загрузку сайта, прерванную в предыдущий раз.

`--span-hosts` - 

## Скачивание при плохой связи

Для того, чтобы `wget` повторял попытки взять файл до тех пор, пока не скачает его целиком, надо указывать ключи `-c` и `-t 0`. 
Первый означает "продолжать качать с того места, где соединение оборвалось (continue), а второй позволяет указать число попыток, 0 -- бесконечно.

Например, чтобы скачать исходные тексты проигрывателя .mp3-файлов X11Amp, можно воспользоваться командой

```bash
wget -c -t 0 http://www.x11amp.bz.nu/files/x11amp-0.9-beta1.1.tar.gz
```

Реально ключи `-c -t 0` стоит указывать практически всегда, кроме как разве что в локальной сети.

## Последующая обработка

В случае скачивания сайта "лидер" я столкнулся с проблемой, что все стили и
ява скрипты скачиваются с номером версии. Как то мне это не особо нравится
и я решил найти все подобные указывани версий и удалить их с помощью регулярых
выражений.

В vscode в поиске регуляркой удаляю лишние приписки к файлам

```js
(?<=[.js|.css])(\?.*?)(?=['|"])
```

И в командной строке делаю групповое переименование. Для него нужна утилита
`rename` - `brew install rename`  
Перед переименованием
удалить ключ `-n` защищающий от случайных переименований.

```bash
# for css
find . -type f -regex ".*\.css.*" -print0 | rename -0 's/(.*.css).*/$1/' -n


# for js
find . -type f -regex ".*\.js.*" -print0 | rename -0 's/(.*.js).*/$1/' -n
```
